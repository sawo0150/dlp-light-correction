amp:           false        # mixed-precision
checkpointing: false        # gradient_checkpoint
accum_steps:   3            # gradient_accumulation (1이면 이 기능 끄는거임.)


grad_clip:           # ⬅︎ 새 섹션
  enable:   true    # true → clipping 실행, false → 끔
  max_norm: 1.0      # torch.nn.utils.clip_grad_norm_ 에 전달
  norm_type: 2       # 2-norm (∞-norm 쓰려면 0 또는 inf)
  
# ─────────────────── Gradient Accumulation Scheduler ───────────────────
#   enable:      true  → milestones에 정의된 규칙 적용
#   milestones:  - epoch: 0   # 해당 epoch(0-base)부터
#                - steps: 10  # 사용할 accum_steps
#   ※ 필요에 맞게 구간을 추가/수정하세요.
grad_accum_scheduler:
  enable: true
  milestones:
    # 초기 탐색
    - epoch: 0
      steps: 2
    # 안정 훈련
    - epoch: 5
      steps: ${training.accum_steps}
    # 세밀 튜닝: max_epochs - 5
    - epoch: ${calc:"${max_epochs} - 5"}
      steps: 4
      
deepspeed:
  enable:       false        # ← 스위치 - true or false
  config:                     # DeepSpeed JSON을 그대로 삽입
    train_micro_batch_size_per_gpu: ${batch_size}
    # └─ GPU 1장 당 forward/backward 1회 실행 시 처리할 샘플 수

    # train_batch_size: 
    #   global batch (= micro * accum * world_size)을 직접 지정하고 싶다면 사용.
    #   지정하지 않으면 auto-infer 됩니다 :contentReference[oaicite:9]{index=9}

    dist_init_required : False # 분산 초기화 (싱글 gpu에는 쓸모 X)

    optimizer:
      type: Adam   # CPU 오프로딩 전용 Adam
      params:
        lr: ${lr}                                # 기존 lr 그대로 사용
        betas: [0.9, 0.999]
        eps: 1e-8
        weight_decay: 0

    zero_optimization:
      stage: 1              # ▶ ZeRO Stage 선택:
                            #    1: optimizer state만 파티셔닝  
                            #    2: optimizer + gradient 파티셔닝  
                            #    3: optimizer + gradient + parameter 파티셔닝
      offload_optimizer:    # ▶ optimizer state를 GPU가 아닌 CPU에 보관
        device: cpu         #    메모리 아끼기 최우선: 1080에 유리

    fp16:
      enabled: false       # ▶ Mixed-precision on/off.
                           # GTX1080은 FP16 속도가 떨어져 FP32 권장
    # ───────── WarmupCosineLR 스케줄러 설정 ─────────
    scheduler:
      type: WarmupCosineLR
      params:
        warmup_min_ratio: 0.001          # 시작 lr = base_lr * ratio
        cos_min_ratio:    0.1          # 최종 lr = base_lr * ratio
        warmup_num_steps: null         # 코드에서 주입
        total_num_steps:  null         # 코드에서 주입