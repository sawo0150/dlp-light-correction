[Task] inverse_1
Current cuda device:  0
[Hydra] training: accum_steps=4 checkpointing=False amp_enabled=True
[Hydra] grad_clip: enable=False max_norm=1.0 norm_type=2
[Hydra-eval] {}
[Hydra-eval] early_enabled=False, stage_table={}
[Hydra-model] model_cfg={'_target_': 'utils.model.unet.Unet', 'in_chans': 1, 'out_chans': 1, 'chans': 32, 'num_pool_layers': 4, 'drop_prob': 0.0}
[Hydra] loss_func ▶ BCEWithLogitsDiceLoss(
  (dice): DiceLoss()
  (bce): BCEWithLogitsLoss()
)
[Hydra] optimizer ▶ AdamW
[Dataset] Loading file index from /home/wosasa/Desktop/25-1_UROP/MiniPack_allinone_COPY_1280/thr_file_index.json...
[Dataset] Parsing metadata for 901 subjects (Indexed Lookup)...
Building Dataset: 100%|████████████████████| 901/901 [00:00<00:00, 87834.23it/s]
[DataLoader] split=train split_source=manifest modes=['binary'] len=1802
[Info] Skip calculating exact effective_steps to save time.
[Hydra] Scheduler ▶ <torch.optim.lr_scheduler.CosineAnnealingLR object at 0x734bc81f5900>
[Resume] None
/home/wosasa/Desktop/25-1_UROP/dlp-light-correction/utils/learning/train_part.py:367: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = GradScaler(enabled=amp_enabled)
[Data] trainpack_root: /home/wosasa/Desktop/25-1_UROP/MiniPack_allinone_COPY_1280
[Data] manifest_csv  : /home/wosasa/Desktop/25-1_UROP/MiniPack_allinone_COPY_1280/manifest.csv
[Hydra-eval] {}
[Hydra-eval] lb_enable=False, lb_every=999999
[Dataset] Loading file index from /home/wosasa/Desktop/25-1_UROP/MiniPack_allinone_COPY_1280/thr_file_index.json...
[Dataset] Parsing metadata for 54 subjects (Indexed Lookup)...
Building Dataset: 100%|██████████████████████| 54/54 [00:00<00:00, 80659.69it/s]
[DataLoader] split=val split_source=manifest modes=['binary'] len=108
Epoch # 0 ............... dlp_inverse_thr2mask_baseline ...............
[Dataset] Loading file index from /home/wosasa/Desktop/25-1_UROP/MiniPack_allinone_COPY_1280/thr_file_index.json...
[Dataset] Parsing metadata for 901 subjects (Indexed Lookup)...
Building Dataset: 100%|████████████████████| 901/901 [00:00<00:00, 85636.84it/s]
[DataLoader] split=train split_source=manifest modes=['binary'] len=1802
Epoch[ 0/50]/:   0%|                         | 0/1802 [00:00<?, ?it/s]/home/wosasa/Desktop/25-1_UROP/dlp-light-correction/utils/learning/train_part.py:90: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast(enabled=amp_enabled):
Epoch[ 0/50]/:  28%|▎| 505/1802 [03:24<08:42,  2.48it/s, dice=0.981, i
